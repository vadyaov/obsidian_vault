### Admission Controller Class

Класс `AdmissionController` используется для ограничения запросов (например, запросов на выборку данных или DML-операций) на основе доступных ресурсов кластера, которые настраиваются в одном или нескольких пулах ресурсов.
Запрос может быть либо допущен к немедленному выполнению, либо поставлен в очередь для последующего выполнения, либо отклонён (либо сразу, либо после помещения в очередь).
Пулы ресурсов могут быть настроены на максимальное количество одновременных запросов, максимальное потребление памяти в масштабе всего кластера, максимальный размер очереди, максимальные и минимальные лимиты памяти на хост для каждого запроса, а также на определение, будет ли параметр запроса `mem_limit` ограничен указанными выше максимальными/минимальными лимитами памяти на хост.
Запросы будут ставиться в очередь, если уже выполняется слишком много запросов или доступной памяти недостаточно. Как только очередь достигнет максимального размера, входящие запросы будут отклоняться. Запросы в очереди будут завершаться по тайм-ауту после настраиваемого интервала времени.

Контроль допуска может выполняться в двух возможных режимах:
- *Распределённый режим*: каждый координатор выполняет контроль допуска независимо, основываясь на информации о решениях по допуску, принятых другими координаторами, которая распространяется через `statestore`. Это традиционная настройка Impala.
- *Режим сервиса допуска*: координаторы настраиваются с помощью параметра `--admission_service_host` для отправки своих запросов в `admissiond`, который выполняет контроль допуска, основываясь на полном представлении о загрузке кластера. Этот режим в настоящее время считается экспериментальным.

В *распределённом режиме*, в зависимости от флага `-is_coordinator`, несколько экземпляров `impalad` могут действовать как координаторы, а также как контроллеры допуска, поэтому некоторую информацию о состоянии кластера необходимо обменивать между `impalad`, чтобы принимать решения о допуске на любом из них.
Каждый координатор поддерживает статистику по каждому пулу и каждому хосту, связанную с запросами, которые он обслуживает как контроллер допуска.
Некоторые из этих локальных статистик допуска, а также некоторые специфические для бэкенда статистики (т.е. статистики, связанные с исполнителем бэкенда, работающим в том же процессе `impalad`), распространяются по кластеру через `statestore` с использованием темы `IMPALA_REQUEST_QUEUE_TOPIC`.
По сути, координаторы отправляют обновления в `statestore`, где статистика допуска отражает нагрузку, а все участвующие бэкенды отправляют обновления, отражающие выполняемую ими нагрузку.

Каждая пара `<impalad, pool>` отправляется в виде обновления по теме с интервалом сердцебиения `statestore`, когда статистика пула изменяется, и обновления по теме от других `impalad` используются для пересчёта совокупной статистики по пулу.
Поскольку статистика по пулу обновляется только при сердцебиении `statestore`, а все решения принимаются на основе кэшированного состояния, совокупная статистика по пулу является лишь приблизительной оценкой. В результате может быть допущено или поставлено в очередь больше запросов, чем заданные пороговые значения, которые, по сути, являются мягкими ограничениями.

#### **Ресурсы памяти:**
Пул может быть настроен таким образом, чтобы позволить запросам, допущенным к этому пулу, "зарезервировать" максимальное количество ресурсов памяти.
Хотя в данный момент Impala не производит фактического "резервирования" памяти при допуске (т.е. нет гарантии выделения памяти для запроса, и возможно перераспределение, при котором несколько запросов полагают, что зарезервировали одну и ту же память), контроллер допуска использует несколько метрик для оценки доступной памяти и допускает запросы только тогда, когда считает, что необходимая память доступна.
В будущем планируется реализовать настоящие резервирования, но это потребует значительных усилий и внесения изменений за пределами контроллера допуска.

Необходимый для допуска объём памяти указывается в параметре запроса `MEM_LIMIT` (либо явно, либо через значение по умолчанию).
Это значение настраивается для каждого хоста. Если лимит памяти не задан, вместо него используется оценка на основе планирования в качестве лимита памяти, а также применяется нижняя граница, основанная на наибольшем начальном резервировании запроса.
Окончательный лимит памяти также ограничивается максимальными/минимальными лимитами памяти, настроенными для пула, с возможностью не применять эти ограничения к параметру запроса `MEM_LIMIT` (если оба этих лимита не заданы, то оценки из планирования не используются в качестве лимита памяти и применяются только для принятия решений о допуске.
Кроме того, оценки больше не будут иметь нижнюю границу, основанную на наибольшем начальном резервировании).

Для допуска запроса должны выполняться следующие четыре условия:
1. Текущая конфигурация пула должна быть допустимой.
2. В данном пуле ресурсов должно быть достаточно доступных ресурсов памяти для запроса. Максимальные ресурсы памяти, настроенные для пула, определяют совокупную память кластера, которую могут резервировать все выполняющиеся запросы в этом пуле. Таким образом, совокупная память, которую необходимо зарезервировать для всех участвующих бэкендов для данного запроса, плюс память уже допущенных запросов, должна быть меньше или равна указанному максимальному количеству ресурсов.
3. Все участвующие бэкенды должны иметь достаточно доступной памяти. Каждый `impalad` имеет лимит памяти на процесс, и это максимальное количество памяти, которое может быть зарезервировано на данном бэкенде.
   3b. (опционально) При использовании групп исполнителей (см. ниже) и допуске к не по умолчанию назначенной группе исполнителей, количество текущих выполняющихся запросов должно быть ниже настроенного максимума для всех участвующих бэкендов.
4. Окончательный лимит памяти на хост должен удовлетворять наибольшему начальному резервированию.

Чтобы допускать запросы на основе этих условий, контроллер допуска учитывает следующие параметры как на уровне каждого хоста, так и на уровне каждого пула:

- **Зарезервированная память (Mem Reserved):** объём памяти, который был сообщён как зарезервированный всеми бэкендами, получаемый из обновлений темы `statestore`.
  Значения, которые отправляются, поступают из трекеров памяти пула в методе `UpdateMemTrackerStats()`, который отражает память, зарезервированную фрагментами, начавшими выполнение. Для запросов, которые выполняются и имеют лимиты памяти, лимит считается зарезервированной памятью, так как использование памяти может достигать этого лимита. В противном случае используется текущее потребление памяти запроса (см. `MemTracker::GetPoolMemReserved()`).
  Совокупные значения по пулу и хосту вычисляются в `UpdateClusterAggregates()`. Это состояние, как только все обновления будут полностью распределены и агрегированы, предоставляет достаточно информации для принятия решений о допуске любым `impalad`. Однако для этого требуется ожидание, пока все допущенные запросы запустят удалённые фрагменты, а затем обновлённое состояние будет распространено через `statestore`.
- **Допущенная память (Mem Admitted):** объём памяти, требуемый для запросов, которые контроллер допуска этого `impalad` допустил (т.е. значение, используемое при допуске, либо лимит памяти, либо оценка). Учёт как на уровне пула, так и на уровне хоста обновляется, когда запросы допускаются и освобождаются (И ВАЖНО: это не делается через `statestore`, поэтому нет задержки, но это не учитывает память, выделенную запросами, допущенными другими `impalad`).
- **Количество допущенных запросов (Num Admitted):** количество запросов, которые были допущены и, следовательно, считаются в данный момент выполняющимися. Обратите внимание, что в настоящее время нет эквивалента для отчёта о зарезервированной памяти, т.е. хосты не сообщают фактическое количество запросов, которые в данный момент выполняются (`IMPALA-8762`). Это мешает использовать несколько координаторов с группами исполнителей.

Как описано, оба механизма учёта — «зарезервированная» и «допущенная» память — имеют свои преимущества и недостатки.
Учёт зарезервированной памяти хорошо работает в стационарном состоянии, т.е. при достаточном времени для распространения обновлений.
Учёт допущенной памяти работает идеально, когда есть один координатор (и, возможно, приемлемо работает при небольшом их количестве).
Для принятия решений о допуске используется максимальное значение из зарезервированной и допущенной памяти, что хорошо работает, когда используется относительно небольшое количество координаторов или, если запросы широко распределены по `impalad`, частота подачи запросов достаточно низкая, чтобы новое состояние успевало обновляться через `statestore`.

#### Освобождение запросов:
Когда запросы завершаются, их необходимо явно освободить из контроллера допуска с помощью методов `ReleaseQuery` и `ReleaseQueryBackends`. Эти методы освобождают выделенную память и уменьшают количество допущенных запросов для пула ресурсов.

В традиционном распределённом режиме контроля допуска требуется, чтобы все бэкенды для запроса были освобождены с помощью `ReleaseQueryBackends`, а затем запрос освобождался с использованием `ReleaseQuery`. Это возможно, так как координатор и контроллер допуска работают в одном процессе.

В режиме сервиса контроля допуска допускается большая гибкость для поддержания отказоустойчивости в случае сбоев RPC между координаторами и `admissiond`. В этом случае правильный учёт ресурсов обеспечивается двумя инвариантами: 
1. Суммарные значения используемых ресурсов всегда соответствуют содержимому `running_queries_`.
2. Любой запрос в конечном итоге будет удалён из `running_queries_`, и все его ресурсы будут освобождены, независимо от любых сбоев.

Рассмотрим несколько возможных случаев сбоев:
- **Сбой RPC для `ReleaseQuery`:** координаторы периодически отправляют список зарегистрированных идентификаторов запросов через RPC сердцебиения, что позволяет контроллеру допуска очищать любые запросы, которых нет в этом списке.
- **Сбой координатора:** сервис контроля допуска использует `statestore` для обнаружения удаления координатора из состава кластера и освобождения всех запросов, которые выполнялись на этом координаторе.
- **Сбой RPC для `ReleaseQueryBackends`:** когда `ReleaseQuery` в конечном итоге вызывается (как гарантируется выше), он автоматически освободит любые оставшиеся бэкенды.

Освобождение бэкендов освобождает выделенную память, используемую этим бэкендом, и уменьшает количество выполняющихся запросов на хосте, где выполнялся этот бэкенд. Освобождение запроса не освобождает выделенную память, оно только уменьшает количество выполняющихся запросов в пуле ресурсов.

### Группы исполнителей:
Исполнители в кластере могут быть назначены в группы исполнителей. Каждый исполнитель может принадлежать только к одной группе. Один пул ресурсов может быть связан с несколькими группами исполнителей. Каждая группа исполнителей принадлежит только одному пулу ресурсов и обслуживает запросы только из этого пула. Таким образом, существуют отношения "1 пул ресурсов : множество групп исполнителей" и "1 группа исполнителей : множество исполнителей".

Исполнители, которые не указывают имя группы при запуске, автоматически добавляются в группу по умолчанию, называемую `DEFAULT_EXECUTOR_GROUP_NAME`. Эта группа не ограничивает количество параллельно выполняемых запросов на каждом хосте и, таким образом, может использоваться несколькими координаторами.

Группы исполнителей автоматически сопоставляются с пулами ресурсов по их имени. Запросы в пуле ресурсов могут выполняться на всех группах исполнителей, чьи имена начинаются с имени пула, разделённого символом '-'. Например, запросы в пуле с именем `q1` могут выполняться на всех группах, начинающихся с `q1-`. Если соответствующих групп для пула ресурсов не найдено и группа по умолчанию не пуста, то используется группа по умолчанию.

Помимо проверок, описанных ранее, допуск в группы исполнителей ограничивается максимальным количеством запросов, которые могут выполняться одновременно на исполнителе (`-admission_control_slots`). Проводится дополнительная проверка, чтобы убедиться, что у каждого исполнителя в группе есть доступный слот для выполнения запроса. Контроллеры допуска включают количество допущенных запросов для каждого исполнителя в обновления `statestore`.

Для поиска группы исполнителей, которая сможет выполнить запрос, контроллер допуска вызывает `FindGroupToAdmitOrReject()`, либо во время первоначальной попытки допуска, либо в `DequeueLoop()`. Если состав кластера изменился, он повторно рассчитывает расписания для всех групп исполнителей и затем пытается допустить запросы, используя список расписаний. Допуск всегда происходит в одном и том же порядке, чтобы группы исполнителей заполнялись до того, как будут рассмотрены следующие. В частности, не предпринимается попыток сбалансировать количество запросов между группами.

#### Пример без использования групп исполнителей:
Рассмотрим кластер из 10 узлов, где на каждом узле по 100 ГБ памяти и пул ресурсов `q1` настроен с совокупным объёмом памяти в 500 ГБ и максимальным лимитом памяти в 40 ГБ. Принимается запрос с опцией `MEM_LIMIT`, установленной на 50 ГБ, и назначением на выполнение на всех бэкендах, что приводит к вызову `SubmitForAdmission()` в иначе тихом кластере. На основе конфигурации пула используется лимит памяти 40 ГБ на узел для этого запроса и всех последующих проверок, необходимых до допуска. `FindGroupToAdmitOrReject()` вычисляет расписание для группы исполнителей по умолчанию и проводит тесты на отказ до вызова `CanAdmitRequest()`, который проверяет количество выполняющихся запросов, а затем вызывает `HasAvailableMemResources()`, чтобы проверить наличие ресурсов памяти.

#### Пример с использованием групп исполнителей:
Рассмотрим кластер с выделенным координатором и двумя группами исполнителей: `default-pool-group-1` и `default-pool-group-2`. Допустим, что для всех исполнителей задано только одно место для допуска (`--admission_control_slots=1`). Поступает запрос с параметром `mt_dop=1`, который передаётся в `SubmitForAdmission()`, вызывающую `FindGroupToAdmitOrReject()`. Затем вызывается `ComputeGroupScheduleStates()`, которая вычисляет расписания для обеих групп исполнителей. Выполняются тесты на отказ, и вызывается `CanAdmitRequest()` для каждого расписания. Группы исполнителей обрабатываются в детерминированном порядке. `CanAdmitRequest()` вызывает `HasAvailableSlots()`, чтобы проверить, могут ли хосты в группе разместить новый запрос в доступных слотах, и если могут, допуск выполняется.

### Поведение очереди:
Когда ресурсы в пуле исчерпаны, каждый координатор, получающий запросы, начинает ставить их в очередь. В каждой отдельной очереди используется принцип FIFO (первый вошёл, первый вышел), однако между контроллерами допуска нет полного порядка запросов, и FIFO-поведение не гарантируется для запросов, отправленных различным координаторам. Когда ресурсы становятся доступными, не осуществляется синхронная координация между узлами для определения, какой из них получает возможность вынуть запросы из очереди и выполнить их допуск. Вместо этого используется простая эвристика, чтобы попытаться вынуть количество запросов, пропорциональное числу ожидающих в каждом контроллере по сравнению с общим количеством запросов во всех контроллерах допуска (например, в impalad). Это ограничивает вероятность избыточного допуска, который может возникнуть при одновременном освобождении большого объема ресурсов. 

Когда на одном узле в очереди находятся запросы для нескольких пулов, контроллер допуска просто проходит по списку `pool_stats_` и пытается вынуть запросы из каждого пула. Это работает для лимита `max_requests`, но является несправедливым в отношении допуска на основе памяти, так как порядок обхода пулов фактически даёт приоритет очередям, находящимся в начале списка. Запросы из различных очередей могут конкурировать за одни и те же ресурсы на конкретных хостах. При этом максимальные ресурсы памяти пула (`#1` в описании выше) не оспариваются.

### Поведение при отмене:
Запрос на допуск `<schedule, admit_outcome>`, отправленный с помощью `SubmitForAdmission()`, может быть отменён проактивно, если установить значение `admit_outcome` в `AdmissionOutcome::CANCELLED`. Это обрабатывается асинхронно методами `SubmitForAdmission()` и `DequeueLoop()`.

### Механизм настройки пула:
Путь к файлам конфигурации пулов указывается с помощью флагов запуска `"fair_scheduler_allocation_path"` и `"llama_site_path"`. Формат для задания конфигураций пулов основан на Yarn и Llama с добавлениями, специфичными для Impala. Запускается служба мониторинга файлов, которая отслеживает изменения в этих файлах. Эти изменения применяются к Impala только при обслуживании нового запроса. Подробности можно найти в классе `RequestPoolService`.